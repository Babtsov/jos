# Lab 4 notes
lab link: https://pdos.csail.mit.edu/6.828/2017/labs/lab4/  
video explanation about APIC: https://youtu.be/rnGVincwk30?t=11m13s  
caffeinated notes version: https://sipb.mit.edu/iap/6.828/lecture-notes/lec-5/    
## Exercise 1
```C
void *
mmio_map_region(physaddr_t pa, size_t size)
{
        static uintptr_t base = MMIOBASE;

        assert(pa % PGSIZE == 0);
        size = ROUNDUP(size, PGSIZE);
        if (base + size > MMIOLIM) {
                panic("Allocation exceeds MMIOLIM: %x", base + size);
        }
        boot_map_region(kern_pgdir, base, size, pa, PTE_PCD | PTE_PWT | PTE_W);
        void *ret_base = (void*)base;
        base += size;
        return ret_base;
}
```
## Exercise 2
_modify your implementation of page_init() in kern/pmap.c to avoid adding the page at MPENTRY_PADDR to the free list, so that we can safely copy and run AP bootstrap code at that physical address._  
here is the updated function:
```C
void
page_init(void)
{
        // 1) Mark the physical page at MPENTRY_PADDR as in use

        // ensure mpentry can fit in a page
        extern unsigned char mpentry_start[], mpentry_end[];
        assert((uintptr_t)(mpentry_end - mpentry_start) <= PGSIZE);

        struct PageInfo* mpentrypg = pa2page(MPENTRY_PADDR);
        mpentrypg->pp_ref = 1;

        //  Mark physical page 0 as in use.
        //  This way we preserve the real-mode IDT and BIOS structures
        //  in case we ever need them.  (Currently we don't, but...)
        pages[0].pp_ref = 1;


        //  2) The rest of base memory up to npages_basemem * PGSIZE
        //     is free.
        for (int i = 0 ; i < npages_basemem; i++) {
                if (pages[i].pp_ref == 1) {
                        continue;
                }
                assert(pages[i].pp_ref == 0);
                pages[i].pp_link = page_free_list;
                page_free_list = &pages[i];
        }

        //  3) Then comes the IO hole [IOPHYSMEM, EXTPHYSMEM), which must
        //     never be allocated.
        uint32_t first_free_pa = (uint32_t) PADDR(boot_alloc(0));
        assert(first_free_pa % PGSIZE == 0);
        int free_pa_pg_indx = first_free_pa / PGSIZE;
        for (int i = npages_basemem ; i < free_pa_pg_indx; i++) {
                pages[i].pp_ref = 1;
                pages[i].pp_link = NULL;
        }

        //  4) Then extended memory [EXTPHYSMEM, ...).
        //     Some of it is in use, some is free. Where is the kernel
        //     in physical memory?  Which pages are already in use for
        //     page tables and other data structures?
        for (int i = free_pa_pg_indx; i < npages; i++) {
                assert(pages[i].pp_ref == 0);
                pages[i].pp_link = page_free_list;
                page_free_list = &pages[i];
        }
}
```

_Compare kern/mpentry.S side by side with boot/boot.S. Bearing in mind that kern/mpentry.S is compiled and linked to run above KERNBASE just like everything else in the kernel, what is the purpose of macro MPBOOTPHYS? Why is it necessary in kern/mpentry.S but not in boot/boot.S? In other words, what could go wrong if it were omitted in kern/mpentry.S?_  

Because `entry.S` is linked to have addresses above KERNBASE, it contains the macro `#define RELOC(x) ((x) - KERNBASE)` in order to translate the addresses generated by the linker (which are meant to be virtual addresses) to physical addresses (load addresses), where the data is actually stored. For example `movl    $(RELOC(entry_pgdir)), %eax` is required to use the `RELOC` macro because we don't want to load the "virtual" address of `entry_pgdir` into %eax but rather the physical (loaded) address. So this `RELOC` macro is useful for "calculating" the physical addresses of the data defined inside the file itself. A similar concept exists in in `mpentry.S` where instead we need to use `MPBOOTPHYS` in order to refer to addresses inside `mpentry.S` itself. Notice that we'd still use `RELOC` inside `mpentry.S` for things that are defined outside of `mpentry.S` such as `entry_pgdir`. If we were to omit it, we'd get virtual addresses while the AP CPU has still not enabled paging. This would cause things to fail.

## Exercise 3
_Modify mem_init_mp() (in kern/pmap.c) to map per-CPU stacks starting at KSTACKTOP, as shown in inc/memlayout.h. The size of each stack is KSTKSIZE bytes plus KSTKGAP bytes of unmapped guard pages._    

Here is the implementation:
```C
static void
mem_init_mp(void)
{
        for (int i = 0; i < NCPU; i++) {
                uintptr_t stacktop = KSTACKTOP - i * (KSTKSIZE + KSTKGAP);
                boot_map_region(kern_pgdir,
                                stacktop - KSTKSIZE,
                                KSTKSIZE,
                                PADDR(percpu_kstacks[i]),
                                PTE_W);
        }


}
```

This made me confused me at first because this implies that we'd be overriding the way we set up the kernel stack prior to SMP support:
```C
        //////////////////////////////////////////////////////////////////////
        // Use the physical memory that 'bootstack' refers to as the kernel
        // stack.  The kernel stack grows down from virtual address KSTACKTOP.
        // We consider the entire range from [KSTACKTOP-PTSIZE, KSTACKTOP)
        // to be the kernel stack, but break this into two pieces:
        //     * [KSTACKTOP-KSTKSIZE, KSTACKTOP) -- backed by physical memory
        //     * [KSTACKTOP-PTSIZE, KSTACKTOP-KSTKSIZE) -- not backed; so if
        //       the kernel overflows its stack, it will fault rather than
        //       overwrite memory.  Known as a "guard page".
        //     Permissions: kernel RW, user NONE

        uintptr_t backed_stack = KSTACKTOP-KSTKSIZE;
        boot_map_region(kern_pgdir, backed_stack, KSTKSIZE, PADDR(bootstack), PTE_W);
```
I was wondering about the following: if we override this mapping inside our new `mem_init_mp` function, once we load the new `kern_pgdir`, wouldn't we mess CPU 0's stack (which starts at KSTACKTOP-KSTKSIZE)? 
After all, `mem_init_mp` makes CPU 0's stack now be mapped to  `percpu_kstacks[0]` (a region currently contains no data) instead of being mapped to `bootstack`, which is where our current stack resides.

The reason why it won't mess it up is because at the time that the new `kern_pgdir` is loaded, the stack pointer actually points to a virtual address outside of `[KSTACKTOP-KSTKSIZE, KSTACKTOP)`, and hence we are safe to modify that mapping. We can confirm this in gdb where we observe that the value of the stack pointer is above `KERNBASE`:
```
(gdb) b pmap.c:248
Breakpoint 1 at 0xf0101e50: file kern/pmap.c, line 248.
(gdb) c
Continuing.
The target architecture is assumed to be i386
=> 0xf0101e50 <mem_init+505>:   mov    -0x10(%ebp),%eax

Breakpoint 1, mem_init () at kern/pmap.c:248
248             lcr0(cr0);
(gdb) i r esp
esp            0xf0124fa0       0xf0124fa0
```
The new kernel stack will be located in `[KSTACKTOP-KSTKSIZE, KSTACKTOP)` only once we switch back from user space to kernel space.

## Exercise 4
_The code in trap_init_percpu() (kern/trap.c) initializes the TSS and TSS descriptor for the BSP. It worked in Lab 3, but is incorrect when running on other CPUs. Change the code so that it can work on all CPUs_  

Notice that it's important that this code runs on both the bootstrap processor and all the application processors. That way, we ensure that all the CPUs have their kernel stack initialized properly.
```C
void
trap_init_percpu(void)
{
        // The example code here sets up the Task State Segment (TSS) and
        // the TSS descriptor for CPU 0. But it is incorrect if we are
        // running on other CPUs because each CPU has its own kernel stack.
        // Fix the code so that it works for all CPUs.
        //
        // Hints:
        //   - The macro "thiscpu" always refers to the current CPU's
        //     struct CpuInfo;
        //   - The ID of the current CPU is given by cpunum() or
        //     thiscpu->cpu_id;
        //   - Use "thiscpu->cpu_ts" as the TSS for the current CPU,
        //     rather than the global "ts" variable;
        //   - Use gdt[(GD_TSS0 >> 3) + i] for CPU i's TSS descriptor;
        //   - You mapped the per-CPU kernel stacks in mem_init_mp()
        //   - Initialize cpu_ts.ts_iomb to prevent unauthorized environments
        //     from doing IO (0 is not the correct value!)
        //
        // ltr sets a 'busy' flag in the TSS selector, so if you
        // accidentally load the same TSS on more than one CPU, you'll
        // get a triple fault.  If you set up an individual CPU's TSS
        // wrong, you may not get a fault until you try to return from
        // user space on that CPU.
        //
        // LAB 4: Your code here:
        struct Taskstate *cpu_ts = &thiscpu->cpu_ts;

        // Setup a TSS so that we get the right stack
        // when we trap to the kernel.
        cpu_ts->ts_esp0 = KSTACKTOP - thiscpu->cpu_id *(KSTKSIZE + KSTKGAP);

        cpu_ts->ts_ss0 = GD_KD;
        cpu_ts->ts_iomb = sizeof(struct Taskstate);

        // Initialize the TSS slot of the gdt.
        gdt[(GD_TSS0 >> 3) + thiscpu->cpu_id] = SEG16(STS_T32A, (uint32_t) (cpu_ts),
                                                sizeof(struct Taskstate) - 1, 0);
        gdt[(GD_TSS0 >> 3) + thiscpu->cpu_id].sd_s = 0;

        // Load the TSS selector (like other segment selectors, the
        // bottom three bits are special; we leave them 0)
        ltr(GD_TSS0 + (thiscpu->cpu_id << 3));

        // Load the IDT
        lidt(&idt_pd);
}
```

## Exercise 5
_Apply the big kernel lock, by calling lock_kernel() and unlock_kernel() at the proper locations._
* _In i386_init(), acquire the lock before the BSP wakes up the other CPUs._  
Notice that `i386_init()` is called by the bootstrap processor, and `lock_kernel()` should be called before `boot_aps();` because this is the function which the bootstrap processor uses to start all the other application processors. We want the bootstrap processor to obtain the lock on the kernel before the application processors will start executing user level processes. In addition to this, when the bootstrap processor starts the application processors, it does it one by one, waiting for one to finish initializing before continuing with the other. We probably don't want for some processors start executing user level environments before we finished to initialized ALL the application processors. 
* _In mp_main(), acquire the lock after initializing the AP, and then call sched_yield() to start running environments on this AP._  
This is necessary to ensure that each application processor is blocked by the lock which was aquired by the bootstrap processor. We need to do this because `sched_yield()` modifies kernel state, so we also don't want different application processors to race against each other and against the bootstrap processor inside that function.
* _In trap(), acquire the lock when trapped from user mode. To determine whether a trap happened in user mode or in kernel mode, check the low bits of the tf_cs._  
We need to lock the kernel if we were trapped from user mode because we might want to modify kernel data structures.
* _In env_run(), release the lock right before switching to user mode. Do not do that too early or too late, otherwise you will experience races or deadlocks._  
As we want to enable the processors to run simulatously when they are executing the user mode code, we release the kernel lock.  
#### In summary, we do the following:
1) aquire the kernel lock when we switch to kernel mode from user mode. the only way this can happen is through an interrupt/trap.
2) release the lock when we leave the kernel mode.
3) some special care is taken during initialization to ensure that the application processors and the bootstap processor don't step on each other's feet.

_It seems that using the big kernel lock guarantees that only one CPU can run the kernel code at a time. Why do we still need separate kernel stacks for each CPU? Describe a scenario in which using a shared kernel stack will go wrong, even with the protection of the big kernel lock._  

Consider the following scenario: a processor is changing from user mode to kernel mode and the hardware pushes information on the kernel stack. Then, before the processor got a chance to do something with that information, another processor also switches from user mode and kernel mode and overrides the information the first processor put on the kernel stack. Notice that the kernel lock doesn't prevent the hardware from overriding the kernel stack.

## Exercise 6
_Implement round-robin scheduling in `sched_yield()`_
```C
void
sched_yield(void)
{
        // Implement simple round-robin scheduling.
        //
        // Search through 'envs' for an ENV_RUNNABLE environment in
        // circular fashion starting just after the env this CPU was
        // last running.  Switch to the first such environment found.
        //
        // If no envs are runnable, but the environment previously
        // running on this CPU is still ENV_RUNNING, it's okay to
        // choose that environment.
        //
        // Never choose an environment that's currently running on
        // another CPU (env_status == ENV_RUNNING). If there are
        // no runnable environments, simply drop through to the code
        // below to halt the cpu.
        // LAB 4: Your code here.

        int index = curenv ? ENVX(curenv->env_id) + 1 : 0;
        bool found = false;
        for (int i = 0; i < NENV; i++) {
                index = (index + i) % NENV;
                if (envs[index].env_status == ENV_RUNNABLE) {
                        found = true;
                        break;
                }
        }

        if (found) {
                env_run(&envs[index]);
        } else if (curenv && curenv->env_status == ENV_RUNNING) {
                env_run(curenv);
        } else {
                sched_halt();
        }
        panic("sched_yield attempted to return");
}
```
_In your implementation of env_run() you should have called lcr3(). Before and after the call to lcr3(), your code makes references (at least it should) to the variable e, the argument to env_run. Upon loading the %cr3 register, the addressing context used by the MMU is instantly changed. But a virtual address (namely e) has meaning relative to a given address context--the address context specifies the physical address to which the virtual address maps. Why can the pointer e be dereferenced both before and after the addressing switch?_  

The variable e is stored above KERNBASE, which means that its memory is mapped to the same location in all environments. All environments share the same kernel mappings because they all initially inherited the mappings of `kern_pgdir`.

_Whenever the kernel switches from one environment to another, it must ensure the old environment'_s registers are saved so they can be restored properly later. Why? Where does this happen?_

It happens in 2 steps. The first step is that `_alltraps` pushes into the kernel stack the registers that the hardware doesn't push automatically (such as the general purpose registers), then inside `trap`, we see the following code:
```C
// Copy trap frame (which is currently on the stack)
// into 'curenv->env_tf', so that running the environment
// will restart at the trap point.
curenv->env_tf = *tf;
```
This basically copies over all the registers into the environment's structure so it can be later restored. This is important because when we switch environemnts, we don't want the new environment to override the registers which were used and relied by the old environment. This way we can have full isolation between the environments.
The way an environment's registers are restored is happening inside `env_pop_tf` which is called by `env_run` whenever we choose to run an environment. `env_pop_tf`. The way this function works is that it first switches the stack pointer to point to the trap frame, and then manually pops all the registers that were pushed into the trap frame, and executes the `iret` instruction which pops the values that were pushed by the hardware and also returns the execution to the user process. 


## Exercise 8
_Implement the sys_env_set_pgfault_upcall system call. Be sure to enable permission checking when looking up the environment ID of the target environment, since this is a "dangerous" system call_  
```C
static int
sys_env_set_pgfault_upcall(envid_t envid, void *func)
{
        // LAB 4: Your code here.
        struct Env *e;
        int err = envid2env(envid, &e, 1);
        if (err < 0) {
                return err;
        }
        e->env_pgfault_upcall = func;
        return 0;
}
```

## Exercise 9
_Implement the code in page_fault_handler in kern/trap.c required to dispatch page faults to the user-mode handler. Be sure to take appropriate precautions when writing into the exception stack. (What happens if the user environment runs out of space on the exception stack?)_  
```C
void
page_fault_handler(struct Trapframe *tf)
{
        uint32_t fault_va;

        // Read processor's CR2 register to find the faulting address
        fault_va = rcr2();

        // Handle kernel-mode page faults.
        if ((tf->tf_cs & 3) == 0) {
                panic("kernel page fault at: %x", fault_va);
        }
        
        if (!curenv->env_pgfault_upcall) {
                goto bad;
        }

        uintptr_t UXSTACKBOTTOM = UXSTACKTOP - PGSIZE;

        uintptr_t tftop = UXSTACKTOP;
        if (tf->tf_esp >= UXSTACKBOTTOM && tf->tf_esp < UXSTACKTOP) {
                // if we are on the user exception stack, then
                // place the next trap frame 4 bytes underneath
                tftop = tf->tf_esp - 4;
        }

        struct UTrapframe *utf =
                (struct UTrapframe *)(tftop - sizeof(struct UTrapframe));

        // ensure user is authorized accessing memory before writing data
        user_mem_assert(curenv, utf, sizeof(struct UTrapframe), PTE_U | PTE_W);

        utf->utf_fault_va = fault_va;
        utf->utf_err = tf->tf_err;
        utf->utf_regs = tf->tf_regs;
        utf->utf_eip = tf->tf_eip;
        utf->utf_eflags = tf->tf_eflags;
        utf->utf_esp = tf->tf_esp;

        // ensure environment returns to pfentry.S with tf stack
        curenv->env_tf.tf_eip = (uintptr_t)curenv->env_pgfault_upcall;
        curenv->env_tf.tf_esp = (uintptr_t)utf;

        env_run(curenv);

 bad:
        // Destroy the environment that caused the fault.
        cprintf("[%08x] user fault va %08x ip %08x\n",
                curenv->env_id, fault_va, tf->tf_eip);
        print_trapframe(tf);
        env_destroy(curenv);
}
```
If we run out of space in the user exception stack, the kernel will destroy the environment (if we don't take this precution, then the kernel itself will page fault, which will halt the system).

## Exercise 10
_Implement the \_pgfault\_upcall routine in lib/pfentry.S. The interesting part is returning to the original point in the user code that caused the page fault. You'll return directly there, without going back through the kernel. The hard part is simultaneously switching stacks and re-loading the EIP._
```assembly
.text
.globl _pgfault_upcall
_pgfault_upcall:
        // Call the C page fault handler.
        pushl %esp                      // function argument: pointer to UTF
        movl _pgfault_handler, %eax
        call *%eax
        addl $4, %esp                   // pop function argument

        movl 40(%esp), %eax // grab trap-time eip
        movl 48(%esp), %ebx // grab trap-time esp
        subl $4, %ebx // reserve a slot to store the eip
        movl %ebx, 48(%esp) // adjust trap-time esp so ret will pop the eip
        mov %eax, (%ebx) // now eip is in the trap-time stack

        addl $8, %esp // pop utf_fault_va and utf_err since we have no use for them

        // Restore the trap-time registers.  After you do this, you
        // can no longer modify any general-purpose registers.
        popal
        
        addl $4, %esp // skip trap-time eip because we can't load it as is
        
        // Restore eflags from the stack.  After you do this, you can
        // no longer use arithmetic operations or anything else that
        // modifies eflags.
        popfl
        
        // Switch back to the adjusted trap-time stack.
        popl %esp
        
        // Return to re-execute the instruction that faulted.
        ret
```
### why do we need to push a an empty 32-bit word between the stack frames?
Let's first think about how we return the exection back to where it page faulted: We need to restore all general purpose registers, the eflags, the stack pointer (%esp), and the instruction pointer (%eip).  

`popl %esp` is the instruction that switches to the destination stack (restores its previous value before the page fault). But how do we restore the %eip? unfortunately, this can't be done directly. instead, we use the stack to reload it. For example, if we have %eip on the stack and we execute the `ret` instruction, the processor will load the %eip with whatever is on the stack (and will pop off that value). 
Since we `ret` after we already restored the stack, we need our %eip to exist right where the restored %esp is pointing towards!. in other words, it needs to be 4 bytes underneath the original target %esp.   

In order for this plan to work, we MUST guaretee that we'll be able to store something 4 bytes underneath the target `%esp`. Usually this is not an issue if our target esp is not in the exception stack, but if it is, then our trap frame (from which we are returning) is exactly below the destination stack (to which we are returning). Therefore, if we don't allocate the 4 bytes as described, writing underneath the target %esp will corrupt the top of our trap frame!


## Exercise 11
_Finish set_pgfault_handler() in lib/pgfault.c_  
```C
void
set_pgfault_handler(void (*handler)(struct UTrapframe *utf))
{
        int r;

        if (_pgfault_handler == 0) {
                // First time through!
                int err = sys_page_alloc(0,
                                         (void *)(UXSTACKTOP - PGSIZE),
                                         PTE_W | PTE_U);
                if (err < 0) {
                        panic("sys_page_alloc: %e", err);
                }
                sys_env_set_pgfault_upcall(0, _pgfault_upcall);
        }

        // Save handler pointer for assembly to call.
        _pgfault_handler = handler;
}
```

## Summary of how user-mode page fault handling works
* the first time `set_pgfault_handler` is called, it allocates a page for the user exception stack. Then, it calls sys_env_set_pgfault_upcall to resiter the *assembly* pgfault entrypoint defined in lib/pfentry.S. Then it sets the user-mode global variable `_pgfault_handler` to point to our custom page fault handler function passed to `set_pgfault_handler`. This global variable `_pgfault_handler` is used by the assembly code inside pfentry.S itself. After all, the kernel must jump to the assembly code, and the assembly code must jump to our handler, so that's why we are not passing the address of the handler itself to the kernel.
* When a page fault occurs, execution jumps to the kernel mode. the kernel starts executing `page_fault_handler`, which detects that we page faulted from user mode.
* The kernel verifies if that environment has a page fault upcall (curenv->env_pgfault_upcall), and that the environment has its user exception stack mapped (as previously mentioned, memory has been mapped there by `set_pgfault_handler`). 
* The kernel copies all register values from the trap frame to the user exception stack, taking into account whether the environment page faulted from the user stack or elsewhere.
* The kernel modifies the trap frame of the environment so it actually returns to the page fault upcall instead of returning back to where the page fault occured. This is done by modifying the trap frame instruction pointer and setting it equal to the address of the page fault upcall (which is written in assembly), and the stack pointer to point to the user exception stack.
* after the trap frame is popped, execution is in the assembly code, which calls our custom page fault handler and then later cleans up after itself.

## why do user/faultalloc and user/faultallocbad behave differently?
### faultalloc
faultalloc calls `cprintf("%s\n", (char*)0xDeadBeef);` and this causes the following:
* the first character of the string located in 0xDeadBeef is attempted to be read by the environment.
* a page fault occurs becase 0xDeadBeef is unmapped
* the page fault handler that we have registered runs. During the run it maps the missing page and also populates the string (using the snprintf function)
* execution returns back from where it faulted, and cprintf can resume printing to the screen because the memory is now mapped and even contains some data.
### faultallocbad
faultallocbad calls `sys_cputs((char*)0xDEADBEEF, 4);` and this causes the following:
* the memory in 0xDEADBEEF is not accessed by the environment, but is instead passed directly to the kernel through a system call. (notice that there was no page fault here and thus the handler has never ran).
* During the handling of this system call, the kernel detects that the memory is unmapped and kills the environment. Therefore, nothing is printed.




### Memory map
```
/*
 * Virtual memory map:                                Permissions
 *                                                    kernel/user
 *
 *    4 Gig -------->  +------------------------------+
 *                     |                              | RW/--
 *                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *                     :              .               :
 *                     :              .               :
 *                     :              .               :
 *                     |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~| RW/--
 *                     |                              | RW/--
 *                     |   Remapped Physical Memory   | RW/--
 *                     |                              | RW/--
 *    KERNBASE, ---->  +------------------------------+ 0xf0000000      --+
 *    KSTACKTOP        |     CPU0's Kernel Stack      | RW/--  KSTKSIZE   |
 *                     | - - - - - - - - - - - - - - -|                   |
 *                     |      Invalid Memory (*)      | --/--  KSTKGAP    |
 *                     +------------------------------+                   |
 *                     |     CPU1's Kernel Stack      | RW/--  KSTKSIZE   |
 *                     | - - - - - - - - - - - - - - -|                 PTSIZE
 *                     |      Invalid Memory (*)      | --/--  KSTKGAP    |
 *                     +------------------------------+                   |
 *                     :              .               :                   |
 *                     :              .               :                   |
 *    MMIOLIM ------>  +------------------------------+ 0xefc00000      --+
 *                     |       Memory-mapped I/O      | RW/--  PTSIZE
 * ULIM, MMIOBASE -->  +------------------------------+ 0xef800000
 *                     |  Cur. Page Table (User R-)   | R-/R-  PTSIZE
 *    UVPT      ---->  +------------------------------+ 0xef400000
 *                     |          RO PAGES            | R-/R-  PTSIZE
 *    UPAGES    ---->  +------------------------------+ 0xef000000
 *                     |           RO ENVS            | R-/R-  PTSIZE
 * UTOP,UENVS ------>  +------------------------------+ 0xeec00000
 * UXSTACKTOP -/       |     User Exception Stack     | RW/RW  PGSIZE
 *                     +------------------------------+ 0xeebff000
 *                     |       Empty Memory (*)       | --/--  PGSIZE
 *    USTACKTOP  --->  +------------------------------+ 0xeebfe000
 *                     |      Normal User Stack       | RW/RW  PGSIZE
 *                     +------------------------------+ 0xeebfd000
 *                     |                              |
 *                     |                              |
 *                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *                     .                              .
 *                     .                              .
 *                     .                              .
 *                     |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|
 *                     |     Program Data & Heap      |
 *    UTEXT -------->  +------------------------------+ 0x00800000
 *    PFTEMP ------->  |       Empty Memory (*)       |        PTSIZE
 *                     |                              |
 *    UTEMP -------->  +------------------------------+ 0x00400000      --+
 *                     |       Empty Memory (*)       |                   |
 *                     | - - - - - - - - - - - - - - -|                   |
 *                     |  User STAB Data (optional)   |                 PTSIZE
 *    USTABDATA ---->  +------------------------------+ 0x00200000        |
 *                     |       Empty Memory (*)       |                   |
 *    0 ------------>  +------------------------------+                 --+
 */
```
